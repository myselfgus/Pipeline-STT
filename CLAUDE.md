# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a Speech-to-Text (STT) Pipeline for processing psychiatric consultation audio recordings. The system transcribes audio files up to 500 MB, performs speaker diarization, and applies PII redaction for LGPD compliance in Brazilian healthcare environments.

## Architecture

The system follows a cloud-native, microservices architecture:

- **Frontend**: Cloudflare Workers for upload orchestration and Voice Activity Detection
- **Storage**: Azure Blob Storage (RA-GRS) for raw audio and transcripts
- **Compute**: Azure Kubernetes Service (AKS) with GPU-enabled Python pods
- **ML Pipeline**: Whisper Large-v3 + WhisperX for transcription and diarization
- **PII Protection**: spaCy with custom Brazilian Portuguese medical entities
- **Messaging**: Azure Service Bus Premium for reliable job queuing
- **Monitoring**: OpenTelemetry with Azure Monitor integration

## Key Technologies

- **Python 3.11+** - Primary runtime for ML workloads
- **PyTorch + CUDA** - GPU acceleration for Whisper models
- **Whisper Large-v3** - OpenAI's speech recognition model
- **WhisperX** - Speaker diarization and alignment
- **spaCy pt_core_news_lg** - Portuguese NLP for PII detection
- **Azure Functions** - Serverless orchestration
- **Kubernetes + KEDA** - Container orchestration and autoscaling

## Development Commands

Environment setup and dependencies have been configured successfully:

```bash
# Python environment setup (COMPLETED)
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Model downloads (COMPLETED)
python -m spacy download pt_core_news_lg  # Portuguese NLP model (568MB)
python -c "import whisper; whisper.load_model('large-v3')"  # Whisper model (2.88GB)

# Environment verification
python test_setup.py  # All tests passing ‚úÖ

# Testing (when implemented)
pytest tests/ --cov=stt --cov-report=xml

# Container builds (when implemented)
docker build -t stt-processor:latest ./stt

# Kubernetes deployment (when implemented)
kubectl apply -f k8s/
```

## Setup Status

‚úÖ **Environment**: Python 3.12 virtual environment created
‚úÖ **Dependencies**: All required packages installed (75+ packages)
‚úÖ **Models**: spaCy pt_core_news_lg + Whisper large-v3 downloaded
‚úÖ **Testing**: Verification script confirms all modules working
‚úÖ **Cloudflare Workers**: Complete pipeline implementation created
‚úÖ **Configuration**: All placeholders replaced with real values
‚ö†Ô∏è  **GPU**: Currently CPU-only mode (CUDA not available in this environment)
üöÄ **Ready for Deployment**: See DEPLOYMENT.md for production setup

## Core Components

### STT Processing Engine (`stt_processor/main.py`)
- Main transcription pipeline with Whisper + WhisperX integration
- GPU memory management and optimization
- PII redaction with Brazilian healthcare-specific entities
- Structured logging with OpenTelemetry tracing

### Blob Trigger Function (`blob_trigger/function_app.py`)
- Azure Functions handler for new audio file uploads
- Validation and Service Bus message queuing
- OpenTelemetry instrumentation for distributed tracing

### Infrastructure (`infrastructure/stt-pipeline.bicep`)
- Azure infrastructure as code using Bicep
- GPU-enabled AKS cluster with dedicated node pools
- Service Bus Premium with session-based queuing
- Key Vault integration for secrets management

## Security & Compliance

### LGPD Requirements
- Data processing must remain within Brazil Southeast region
- All PII must be automatically redacted from transcripts
- Audit logging required for all data processing activities
- Data subject rights implementation (export/delete capabilities)

### Zero-Trust Architecture
- Private AKS cluster with network policies
- Service mesh with mutual TLS
- Key Vault integration for secrets
- Kubernetes RBAC with least privilege

## Performance Considerations

- **GPU Memory**: Models require ~8-16GB VRAM (Standard_NV8as_v4 nodes)
- **Processing Time**: ~10 minutes for 500MB audio files
- **Scaling**: KEDA-based autoscaling on Service Bus queue length
- **Caching**: Redis caching for model inference results

## Monitoring & Observability

- **OpenTelemetry**: Distributed tracing across all components
- **Custom Metrics**: Transcription duration, word error rate, GPU utilization
- **Health Checks**: GPU availability, model loading, memory usage
- **Alerts**: Queue backlog, processing failures, compliance violations

## Testing Strategy

- **Unit Tests**: Individual component testing with mocked dependencies
- **Integration Tests**: End-to-end pipeline validation
- **Performance Tests**: GPU memory usage and processing time validation
- **Security Tests**: PII redaction accuracy and LGPD compliance

## File Structure (Current)

```
/
‚îú‚îÄ‚îÄ venv/                   # Python virtual environment (CREATED)
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies (CREATED - 75+ packages)
‚îú‚îÄ‚îÄ test_setup.py          # Environment verification script (CREATED)
‚îú‚îÄ‚îÄ CLAUDE.md              # Project documentation and guidance
‚îú‚îÄ‚îÄ stt_pipeline_python_docs.md  # Detailed technical documentation
‚îú‚îÄ‚îÄ stt_processor/          # Main STT processing engine (TO BE CREATED)
‚îú‚îÄ‚îÄ blob_trigger/           # Azure Functions blob trigger (TO BE CREATED)
‚îú‚îÄ‚îÄ infrastructure/         # Bicep templates for Azure resources (TO BE CREATED)
‚îú‚îÄ‚îÄ k8s/                   # Kubernetes manifests (TO BE CREATED)
‚îú‚îÄ‚îÄ tests/                 # Test suites (TO BE CREATED)
‚îî‚îÄ‚îÄ telemetry/             # OpenTelemetry configuration (TO BE CREATED)
```

## Development Notes

- Always test PII redaction thoroughly with Brazilian Portuguese medical terms
- GPU memory management is critical - use context managers for cleanup
- All processing must include OpenTelemetry spans for observability
- LGPD audit logging is mandatory for all data operations
- Use structured logging (structlog) for consistent log format